Hybrid Knowledge-Based Expert System (Demo)

This repository provides a public, reproducible demonstration of a hybrid AI pipeline for classifying informal food descriptions.

The system combines:
- BERT-based contextual text embeddings
- Prototype-based classification
- PPO-based embedding refinement using Stable-Baselines3
- Fuzzy and symbolic knowledge-based rule adjustments
- An online inference API with logging and offline update support

This repository reflects the actual research architecture, while replacing private experimental data with a small synthetic dataset for safe public release.


Data and Privacy

The dataset in data/RefinedData.parquet is synthetic and intended only for demonstration.
The class labels must match exactly those defined in config/class_order.json.
Users are expected to generate their own data splits locally by running the provided split script.

Do not commit real user data, private datasets, logs, or model checkpoints.


Repository Structure

src/
  Core training, inference, evaluation, and server scripts

config/
  Class order, fuzzy terms, symbolic rules

data/
  Synthetic demo dataset in Parquet format

logs/
  Created at runtime by the server and not committed

reports/
  Generated by evaluation and mining scripts and not committed


Setup

Create a virtual environment.

Windows PowerShell:
python -m venv .venv
.\.venv\Scripts\Activate.ps1

macOS or Linux:
python -m venv .venv
source .venv/bin/activate

Install dependencies:
pip install -r requirements.txt


Usage Workflow

All paths are relative to the repository root.
No hard-coded local paths are used.


Step 1: Generate train, validation, and test splits

python src/splitter.py

This creates split files under:
data/splits/


Step 2: Train BERT encoder and PPO policy

python src/ReinforcementLearningWithRefinedData_parquet.py

This step:
- trains the BERT encoder
- constructs class prototypes
- trains the PPO embedding refinement policy


Step 3: Run baseline and hybrid evaluations

python src/eval_end_to_end.py --variant bert_proto
python src/eval_end_to_end.py --variant ppo
python src/eval_end_to_end.py --variant ppo_kbs
python src/eval_end_to_end.py --variant kbs_fuzzy_only
python src/eval_end_to_end.py --variant kbs_sym_only

Evaluation outputs are written to reports/.


Step 4: Run the API server

python -m uvicorn src.server:app --reload --host 127.0.0.1 --port 8000

Open in browser:
http://127.0.0.1:8000/docs


Step 5: Ingest prediction and feedback logs (optional)

python src/ingest_logs.py --predict logs/predict.jsonl --feedback logs/feedback.jsonl


Step 6: Mine fuzzy and symbolic rule candidates (optional)

python src/mine_kbs_candidates.py
python src/mine_kbs_candidates.py --apply-fuzzy --apply-symbolic


Step 7: Reload rules in the running server

curl -X POST http://127.0.0.1:8000/admin/reload


Step 8: Offline PPO retraining (optional)

python src/offline_retrain.py


Notes

This repository is a research demonstration, not a production system.
The architecture is designed to support continual learning, interpretability, and robust handling of informal language.
Users may replace the synthetic dataset with their own Parquet data, provided the schema and class labels remain consistent.
